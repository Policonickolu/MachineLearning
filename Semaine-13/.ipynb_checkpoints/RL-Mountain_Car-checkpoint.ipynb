{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 13 - Reinforcement Learning, Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice de cette semaine, nous utiliserons le module *Gym* d'OpenAI. Gym implémente divers environnements compatibles avec des problèmes de RL pour permettre à la communauté scientifique de s'y entraîner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le lien vers la documentation de *Gym*:   https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (0.12.1)\n",
      "Requirement already satisfied: requests>=2.0 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: scipy in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: six in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (1.24.1)\n",
      "Requirement already satisfied: future in /sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans MountainCar, il faire à apprendre au pilote de la voiture comment monter au sommet de la montagne. Mais le moteur n'a pas assez de puissance pour y arriver d'un coup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pilote dispose de 3 actions possibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez différentes valeurs pour la variable \"action\" afin de découvrir comment est encodé le choix de l'action pour le passer à l'environnement au travers de la méthode step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les observations et les états"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement fournit à l'agent deux valeurs continues comme observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des valeurs correspond à la position, l'autre à la vitesse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les observations sont renvoyées par deux différentes méthodes de l'environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41584584  0.        ]\n",
      "[-4.15639982e-01  2.05854321e-04]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)\n",
    "observation, reward, done, info = env.step(action)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire du q-learning, il faut un nombre fini d'état, donc nos valeurs continues doivent devenir discrètes. Écrivez une fonction qui divise l'échelle de variation de chaque valeur en 20 segments égaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = 20\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    min_position, min_speed = env.observation_space.low\n",
    "    max_position, max_speed = env.observation_space.high\n",
    "    position, speed = obs\n",
    "    \n",
    "    position_delta = (max_position - min_position) / n_segments\n",
    "    speed_delta = (max_speed - min_speed) / n_segments\n",
    "    position_segm = int((position-min_position)/position_delta)\n",
    "    speed_segm = int((speed-min_speed)/speed_delta)\n",
    "    \n",
    "    state = position_segm, speed_segm\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre fonction: Faites-lui transformer l'état renvoyé par env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "print(obs_to_state(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par souci de simplicité, nous n'allons pas implémenter l'agent comme objet, mais néanmoins nous implémenterons sa Policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une table de q-valeurs qui peut contenir une q-valeur distincte pour chaque action possible, dans tous les états imaginables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.25091976  0.90142861  0.46398788]\n",
      "  [ 0.19731697 -0.68796272 -0.68801096]\n",
      "  [-0.88383278  0.73235229  0.20223002]\n",
      "  ...\n",
      "  [ 0.55026565  0.87899788  0.7896547 ]\n",
      "  [ 0.19579996  0.84374847 -0.823015  ]\n",
      "  [-0.60803428 -0.90954542 -0.34933934]]\n",
      "\n",
      " [[-0.22264542 -0.45730194  0.65747502]\n",
      "  [-0.28649335 -0.43813098  0.08539217]\n",
      "  [-0.71815155  0.60439396 -0.85089871]\n",
      "  ...\n",
      "  [-0.67755743  0.8593953   0.61624076]\n",
      "  [ 0.26680751  0.74292118  0.60734415]\n",
      "  [-0.62685988  0.785118    0.07868448]]\n",
      "\n",
      " [[ 0.61488031  0.7921826  -0.36399305]\n",
      "  [-0.77989615 -0.54412967 -0.14578442]\n",
      "  [ 0.63602953  0.72146117 -0.98609574]\n",
      "  ...\n",
      "  [-0.96682434  0.02418612 -0.54700845]\n",
      "  [ 0.29034558 -0.65126714  0.38187548]\n",
      "  [-0.22652931  0.87345998 -0.72495811]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.39484053 -0.40530199  0.84879239]\n",
      "  [ 0.94211649  0.88853298 -0.05157157]\n",
      "  [ 0.7240853   0.6890988  -0.36179905]\n",
      "  ...\n",
      "  [-0.5327845  -0.91581362 -0.96425213]\n",
      "  [ 0.97544478 -0.14445373 -0.23134671]\n",
      "  [ 0.35929457 -0.56349222  0.89992237]]\n",
      "\n",
      " [[ 0.57269003 -0.821178   -0.16483845]\n",
      "  [ 0.75823662  0.88946404 -0.06519698]\n",
      "  [ 0.22682278 -0.66593211  0.98233725]\n",
      "  ...\n",
      "  [-0.00824853 -0.83790757 -0.5596336 ]\n",
      "  [ 0.36651753 -0.84773828  0.70241383]\n",
      "  [-0.00970695 -0.03882685  0.18481557]]\n",
      "\n",
      " [[ 0.64936193 -0.30438158  0.35603231]\n",
      "  [ 0.13146393 -0.46594346  0.75725997]\n",
      "  [ 0.59485204  0.31690367  0.70116346]\n",
      "  ...\n",
      "  [ 0.16434092 -0.30823389  0.24183104]\n",
      "  [-0.90851593  0.74307361  0.94697794]\n",
      "  [ 0.93775571  0.49930366 -0.73982752]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dimensions = (n_segments, env.action_space.n) \n",
    "q_table = np.random.uniform(low=-1, high=1, size=dimensions)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez la Greedy policy, qui utilise la q_table pour décider de la meilleure action à choisir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state):\n",
    "    action = q_table[state].index(q_table[state].max)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentez maintenant la Epsilon-greedy policy, qui ajoute une probabilité *epsilon* que l'action soit choisie aléatoirement  \n",
    "_**Indice:**_ np.random.choice(n) choisira aléatoirement un nombre de 0 à n exclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = greedy_policy(state)\n",
    "    else:\n",
    "        action = random.choice(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "episodes_max = 5000\n",
    "lr = 0.2\n",
    "gamma = 0.9\n",
    "\n",
    "# Epsilon will diminish over the episodes to increase exploitation\n",
    "start_epsilon = 0.8\n",
    "final_epsilon = 0\n",
    "epsilon_reduction = (start_epsilon - final_epsilon) / episodes_max\n",
    "epsilon = start_epsilon\n",
    "\n",
    "reward_list = []\n",
    "avg_reward_list = []\n",
    "\n",
    "# Train for 'episodes_max' episodes\n",
    "for episode in range(episodes_max):\n",
    "    \n",
    "    # Initialize parameters of the episode\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state1 = obs_to_state(env.reset())\n",
    "    \n",
    "    # Each episode runs for a maximum of 200 timesteps, at whith point the Environment returns done = True\n",
    "    while done is False:\n",
    "        \n",
    "        # The agent chooses an action\n",
    "        action = greedy_policy(state1)\n",
    "        \n",
    "        # The action is given to the environment, which returns a new state, a reward, and some other info\n",
    "        # (see documentation https://gym.openai.com/docs/)\n",
    "    \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state2 = obs_to_state(observation)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update q_table\n",
    "        if done:\n",
    "            \"\"\"Quelle est la q-valeur de la toute dernière action???\"\"\"\n",
    "            q_table[state1][action] = \"\"\"À compléter\"\"\"\n",
    "        else:\n",
    "            \"\"\"Hmmmm... On dirait qu'on va s'amuser avec l'équation de Bellman!!\"\"\"\n",
    "            q_table[state1][action] = \"\"\"À compléter\"\"\"\n",
    "        \n",
    "        # The new state becomes state1 of the next loop\n",
    "        state1 = state2\n",
    "    \n",
    "    # Epsilon decay\n",
    "    epsilon -= epsilon_reduction\n",
    "    \n",
    "    # Display rewards\n",
    "    reward_list.append(total_reward)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(reward_list)\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        reward_list = []\n",
    "        print('Iteration #{} -- Average reward = {}'.format(episode+1, avg_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La mise à l'épreuve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant voir notre agent en action avec la Policy optimale. Quelle fonction utilisera-t-on pour sélectionner chaque action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = obs_to_state(env.reset())\n",
    "done = False\n",
    "while done is False:\n",
    "    env.render()\n",
    "    action = \"\"\"À compléter\"\"\"\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évolution de l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un petit graphique pour voir l'évolution du Return moyen de chaque tranche de 100 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(100*(np.arange(len(avg_reward_list)) + 1), avg_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
