{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 14 - Reinforcement Learning, Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice de cette semaine, nous ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le lien vers la documentation de *Gym*:   https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Pong, il faut apprendre au joueur de droite à annalyser la trajectoire de la balle et à déplacer la raquette pour renvoyer la balle dans le camps adverse à l'endroit où il ne peut la renvoyer et gagner des points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le joueur dispose de 6 actions possibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez différentes valeurs pour la variable \"action\" afin de découvrir comment est encodé le choix de l'action pour le passer à l'environnement au travers de la méthode step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les observations et les états"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement fournit à l'agent une image du jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(210, 160, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les observations sont renvoyées par deux différentes méthodes de l'environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1262b7978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(action)\n",
    "plt.imshow(observation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "    \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take 1D float array of rewards and compute discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            running_add = running_add * gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "    \"\"\"\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    \n",
    "    \"\"\"\n",
    "        h = np.dot(model['W1'], x)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        logp = np.dot(model['W2'], h)\n",
    "        p = sigmoid(logp)\n",
    "    \"\"\"\n",
    "    return p, h # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward pass. (eph is array of intermediate hidden states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \n",
    "    \"\"\"\n",
    "        dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "        dh = np.outer(epdlogp, model['W2'])\n",
    "        dh[eph <= 0] = 0 # backpro prelu\n",
    "        dW1 = np.dot(dh.T, epx)\n",
    "    \"\"\"\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the observation, set input to network to be difference image\n",
    "def difference_image():\n",
    "    global prev_x\n",
    "    cur_x = prepro(observation)\n",
    "    diff_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "    return diff_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward the policy network and sample an action from the returned probability\n",
    "def get_action(x):\n",
    "    aprob, h = policy_forward(x)\n",
    "    \"\"\"\n",
    "        action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "    \"\"\"\n",
    "    return action, aprob, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record various intermediates (needed later for backprop)\n",
    "def record_history(x, action, aprob, h):\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state for fitting\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "def get_history():\n",
    "    global xs, hs, dlogps, drs\n",
    "    \"\"\"\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "    \"\"\"\n",
    "    return epx, eph, epdlogp, epr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the discounted reward backwards through time\n",
    "def standardize_reward(epr):\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    \"\"\"\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "    \"\"\"\n",
    "    return discounted_epr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizer function, you don't have to understand it, works like Adam\"\"\"\n",
    "def rmsprop_update():\n",
    "    for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boring book-keeping\n",
    "def book_keeping(episode_number):\n",
    "    global running_reward, reward_sum\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %f. running mean: %f', reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n",
      "resetting env. episode reward total was %f. running mean: %f 0.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-c55ea0008f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mbook_keeping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mprev_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# return: (states, observations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sgoinfre/goinfre/Perso/hben-yah/miniconda3/lib/python3.7/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mreset_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetLegalActionSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "# import numpy as np\n",
    "# import _pickle as pickle\n",
    "# import gym\n",
    "\n",
    "# # hyperparameters\n",
    "# H = 200 # number of hidden layer neurons\n",
    "# batch_size = 10 # every how many episodes to do a param update?\n",
    "# learning_rate = 1e-4\n",
    "# gamma = 0.99 # discount factor for reward\n",
    "# decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "# resume = False # resume from previous checkpoint?\n",
    "# render = True\n",
    "\n",
    "# model initialization\n",
    "# D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "# if resume:\n",
    "#   model = pickle.load(open('save.p', 'rb'))\n",
    "# else:\n",
    "#   model = {}\n",
    "#   model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "#   model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "# grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "# rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "# def sigmoid(x): \n",
    "#   return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "# def prepro(I):\n",
    "#   \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "#   I = I[35:195] # crop\n",
    "#   I = I[::2,::2,0] # downsample by factor of 2\n",
    "#   I[I == 144] = 0 # erase background (background type 1)\n",
    "#   I[I == 109] = 0 # erase background (background type 2)\n",
    "#   I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "#   return I.astype(np.float).ravel()\n",
    "\n",
    "# def discount_rewards(r):\n",
    "#   \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "#   discounted_r = np.zeros_like(r)\n",
    "#   running_add = 0\n",
    "#   for t in reversed(range(0, r.size)):\n",
    "#     if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "#     running_add = running_add * gamma + r[t]\n",
    "#     discounted_r[t] = running_add\n",
    "#   return discounted_r\n",
    "\n",
    "# def policy_forward(x):\n",
    "#   h = np.dot(model['W1'], x)\n",
    "#   h[h<0] = 0 # ReLU nonlinearity\n",
    "#   logp = np.dot(model['W2'], h)\n",
    "#   p = sigmoid(logp)\n",
    "#   return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "# def policy_backward(eph, epdlogp):\n",
    "#   \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "#   dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "#   dh = np.outer(epdlogp, model['W2'])\n",
    "#   dh[eph <= 0] = 0 # backpro prelu\n",
    "#   dW1 = np.dot(dh.T, epx)\n",
    "#   return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "# env = gym.make(\"Pong-v0\")\n",
    "# observation = env.reset()\n",
    "# prev_x = None # used in computing the difference frame\n",
    "# xs,hs,dlogps,drs = [],[],[],[]\n",
    "# running_reward = None\n",
    "# reward_sum = 0\n",
    "# episode_number = 0\n",
    "\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "#     # preprocess the observation, set input to network to be difference image\n",
    "#     cur_x = prepro(observation)\n",
    "#     x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "#     prev_x = cur_x\n",
    "    x = difference_image()\n",
    "\n",
    "#     # forward the policy network and sample an action from the returned probability\n",
    "#     aprob, h = policy_forward(x)\n",
    "#     action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "    action, aprob, h = get_action(x)\n",
    "\n",
    "#     # record various intermediates (needed later for backprop)\n",
    "#     xs.append(x) # observation\n",
    "#     hs.append(h) # hidden state\n",
    "#     y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "#     dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    record_history(x, action, aprob, h)\n",
    "    \n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "#     # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "#     epx = np.vstack(xs)\n",
    "#     eph = np.vstack(hs)\n",
    "#     epdlogp = np.vstack(dlogps)\n",
    "#     epr = np.vstack(drs)\n",
    "#     xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "    epx, eph, epdlogp, epr = get_history()\n",
    "\n",
    "#     # compute the discounted reward backwards through time\n",
    "#     discounted_epr = discount_rewards(epr)\n",
    "#     # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "#     discounted_epr -= np.mean(discounted_epr)\n",
    "#     discounted_epr /= np.std(discounted_epr)\n",
    "    discounted_epr = standardize_reward(epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "        rmsprop_update()\n",
    "\n",
    "    book_keeping(episode_number)\n",
    "\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print('ep %d: game finished, reward: %f', episode_number, reward, '' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
