{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semaine 14 - Reinforcement Learning, Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice de cette semaine, nous ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le lien vers la documentation de *Gym*:   https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Pong, il faut apprendre au joueur de droite à annalyser la trajectoire de la balle et à déplacer la raquette pour renvoyer la balle dans le camps adverse à l'endroit où il ne peut la renvoyer et gagner des points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le joueur dispose de 6 actions possibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez différentes valeurs pour la variable \"action\" afin de découvrir comment est encodé le choix de l'action pour le passer à l'environnement au travers de la méthode step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "for timestep in range(200):\n",
    "    env.render()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les observations et les états"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement fournit à l'agent une image du jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(210, 160, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les observations sont renvoyées par deux différentes méthodes de l'environnement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124dca358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADk5JREFUeJzt3X+s1fV9x/Hnq1g0ka7ywxGDOMDQJmg2aolzazVuzlbpUnR/WMi0tDO7mkBS0i4LarKaJU26rmjSbKPBSIrToW7USlLaykhT06xawVIEFQWEyA1Ce23U1aYWeO+P7+e2h8u93HPP+xzP95y9HsnN/Z7P+X7P9/0N95XvD77f91FEYGate0+3CzDrdQ6RWZJDZJbkEJklOURmSQ6RWVLHQiTpOkl7Je2TtLpT6zHrNnXi/4kkTQJeAq4FDgPPAMsi4vm2r8ysyzq1J7oc2BcRByLiHeBhYEmH1mXWVWd16HNnAa82vD4M/PFYM0s64+5w9u9NalNZZs179c0TP4+I88ebr1MhGpekAWAAYOo57+GLV7+/W6WM6to//ZMJL7P1f37UgUp63/bPf2LCyyy659sdqGRiVn33F4eama9Th3ODwOyG1xeWsd+KiHURsSgiFk2ZrA6VYdZ5nQrRM8B8SXMlTQaWAps7tC6zrurI4VxEHJe0EvgeMAlYHxF7OrEus27r2DlRRGwBtnTq899to53vtHLeZKOf77Ry3lQXvmPBLMkhMktyiMySHCKzJIfILMkhMktyiMySHCKzJIfILMkhMktyiMySHCKzpK49lNdrfLNp+/Tyzaaj8Z7ILMkhMktyiMySfE40BjcdaZ86NB3ppJb3RJJmS/q+pOcl7ZH0uTJ+t6RBSTvLz+L2lWtWP5k90XHgCxHxrKT3ATskbS3v3RsRX82XZ1Z/LYcoIo4AR8r0W5JeoGraOGHT5l7KzQ9ua7UUs45YNWNGU/O15cKCpDnAh4Cny9BKSbskrZc0tR3rMKurdIgkTQE2Aasi4k1gLXAxsJBqT7VmjOUGJG2XtH1oaChbhlnXpEIk6b1UAXooIr4JEBFHI+JERJwE7qNqbn+axg6o06dPz5Rh1lWZq3MC7gdeiIh7GsYvaJjtRmB36+WZ1V/m6txHgFuA5yTtLGN3AsskLQQCOAjclqrQrOYyV+d+CIzWib5vup6aNcO3/ZglOURmSQ6RWVItbkB9/ZXdPHjz/G6XYdYS74nMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMktJ3cUs6CLwFnACOR8QiSdOAR4A5VI+I3xQRv8iuy6yO2rUn+rOIWBgRi8rr1cC2iJgPbCuvzfpSpw7nlgAbyvQG4IYOrces69oRogCekLRD0kAZm1naDAO8Bsxsw3rMaqkdT7Z+NCIGJf0+sFXSi41vRkRIipELlcANAEw9x9c3rHel/3ojYrD8PgY8RtXx9OhwE8fy+9goy/22A+qUyaN13jLrDdk2wueWr1VB0rnAx6g6nm4GlpfZlgOPZ9ZjVmfZw7mZwGNVR2HOAv4jIr4r6RngUUm3AoeAm5LrMautVIgi4gDwR6OMDwHXZD7brFf4jN4sySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsqeUnWyV9kKrL6bB5wD8A5wF/C/ysjN8ZEVtartCs5loOUUTsBRYCSJoEDFJ1+/kscG9EfLUtFZrVXLsO564B9kfEoTZ9nlnPaFeIlgIbG16vlLRL0npJU9u0DrNaSodI0mTgk8B/lqG1wMVUh3pHgDVjLDcgabuk7f/7zmkNUs16Rjv2RNcDz0bEUYCIOBoRJyLiJHAfVUfU07gDqvWLdoRoGQ2HcsPtg4sbqTqimvWtVPPG0jr4WuC2huGvSFpI9W0RB0e8Z9Z3sh1QfwlMHzF2S6oisx7jOxbMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLSj2UZ1YX2z//iVNeL7rn2+/aupvaE5XWV8ck7W4YmyZpq6SXy++pZVySviZpX2mbdVmnijerg2YP574BXDdibDWwLSLmA9vKa6i6/8wvPwNULbTM+lZTIYqIJ4HXRwwvATaU6Q3ADQ3jD0TlKeC8ER2AzPpK5sLCzIg4UqZfA2aW6VnAqw3zHS5jp3DzRusXbbk6FxFB1SJrIsu4eaP1hUyIjg4fppXfx8r4IDC7Yb4Ly5hZX8qEaDOwvEwvBx5vGP90uUp3BfBGw2GfWd9p6v+JJG0ErgZmSDoMfBH4MvCopFuBQ8BNZfYtwGJgH/A21fcVmfWtpkIUEcvGeOuaUeYNYEWmKLNe4tt+zJIcIrMkh8gsySEyS3KIzJIcIrMkP09kfeHdfH5oJO+JzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrOkcUM0RvfTf5b0Yulw+pik88r4HEm/krSz/Hy9k8Wb1UEze6JvcHr3063ApRHxh8BLwB0N7+2PiIXl5/b2lGlWX+OGaLTupxHxREQcLy+fomqLZfb/UjvOif4G+E7D67mSfiLpB5KuHGshd0C1fpF6FELSXcBx4KEydAS4KCKGJH0Y+JakSyLizZHLRsQ6YB3ARe8/yymyntXynkjSZ4C/BP66tMkiIn4dEUNlegewH/hAG+o0q62WQiTpOuDvgU9GxNsN4+dLmlSm51F9vcqBdhRqVlfjHs6N0f30DuBsYKskgKfKlbirgH+U9BvgJHB7RIz8ShazvjJuiMbofnr/GPNuAjZlizLrJb5jwSzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCyp1Q6od0sabOh0urjhvTsk7ZO0V9LHO1W4WV202gEV4N6GTqdbACQtAJYCl5Rl/m24cYlZv2qpA+oZLAEeLq2zXgH2AZcn6jOrvcw50crS0H69pKllbBbwasM8h8vYadwB1fpFqyFaC1wMLKTqerpmoh8QEesiYlFELJoyWS2WYdZ9LYUoIo5GxImIOAncx+8O2QaB2Q2zXljGzPpWqx1QL2h4eSMwfOVuM7BU0tmS5lJ1QP1xrkSzemu1A+rVkhYCARwEbgOIiD2SHgWep2p0vyIiTnSmdLN6aGsH1DL/l4AvZYoy6yW+Y8EsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrOkVps3PtLQuPGgpJ1lfI6kXzW89/VOFm9WB+M+2UrVvPFfgAeGByLiU8PTktYAbzTMvz8iFrarQLO6a+bx8CclzRntPUkCbgL+vL1lmfWO7DnRlcDRiHi5YWyupJ9I+oGkK5Ofb1Z7zRzOnckyYGPD6yPARRExJOnDwLckXRIRb45cUNIAMAAw9Rxf37De1fJfr6SzgL8CHhkeKz24h8r0DmA/8IHRlncHVOsXmV3AXwAvRsTh4QFJ5w9/C4SkeVTNGw/kSjSrt2YucW8EfgR8UNJhSbeWt5Zy6qEcwFXArnLJ+7+A2yOi2W+UMOtJrTZvJCI+M8rYJmBTviyz3uEzerMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsySEyS3KIzJIcIrMkh8gsKft4eFtMm3spNz+4rdtlmJ1i1YwZTc3nPZFZkkNkltTM4+GzJX1f0vOS9kj6XBmfJmmrpJfL76llXJK+JmmfpF2SLuv0Rph1UzN7ouPAFyJiAXAFsELSAmA1sC0i5gPbymuA66kalMynaom1tu1Vm9XIuCGKiCMR8WyZfgt4AZgFLAE2lNk2ADeU6SXAA1F5CjhP0gVtr9ysJiZ0TlTaCX8IeBqYGRFHyluvATPL9Czg1YbFDpcxs77UdIgkTaHq5LNqZEfTiAggJrJiSQOStkvaPjQ0NJFFzWqlqRBJei9VgB6KiG+W4aPDh2nl97EyPgjMblj8wjJ2isYOqNOnT2+1frOua+bqnID7gRci4p6GtzYDy8v0cuDxhvFPl6t0VwBvNBz2mfWdZu5Y+AhwC/Dc8Jd5AXcCXwYeLR1RD1F9xQrAFmAxsA94G/hsWys2q5lmOqD+EBir4/w1o8wfwIpkXWY9w3csmCU5RGZJDpFZkkNkluQQmSWpupjW5SKknwG/BH7e7VraaAb9sz39tC3Q/Pb8QUScP95MtQgRgKTtEbGo23W0Sz9tTz9tC7R/e3w4Z5bkEJkl1SlE67pdQJv10/b007ZAm7enNudEZr2qTnsis57U9RBJuk7S3tLYZPX4S9SPpIOSnpO0U9L2MjZqI5c6krRe0jFJuxvGerYRzRjbc7ekwfJvtFPS4ob37ijbs1fSxye8wojo2g8wCdgPzAMmAz8FFnSzpha34yAwY8TYV4DVZXo18E/drvMM9V8FXAbsHq9+qsdcvkN1Z/8VwNPdrr/J7bkb+LtR5l1Q/u7OBuaWv8dJE1lft/dElwP7IuJARLwDPEzV6KQfjNXIpXYi4kng9RHDPduIZoztGcsS4OGI+HVEvEL1HNzlE1lft0PUL01NAnhC0g5JA2VsrEYuvaIfG9GsLIeg6xsOr9Pb0+0Q9YuPRsRlVD33Vki6qvHNqI4bevYyaK/XX6wFLgYWAkeANe364G6HqKmmJnUXEYPl9zHgMarDgbEaufSKVCOauomIoxFxIiJOAvfxu0O29PZ0O0TPAPMlzZU0GVhK1eikZ0g6V9L7hqeBjwG7GbuRS6/oq0Y0I87bbqT6N4Jqe5ZKOlvSXKrOvT+e0IfX4ErKYuAlqqsid3W7nhbqn0d1deenwJ7hbQCmU7VXfhn4b2Bat2s9wzZspDrE+Q3VOcGtY9VPdVXuX8u/13PAom7X3+T2/Hupd1cJzgUN899VtmcvcP1E1+c7FsySun04Z9bzHCKzJIfILMkhMktyiMySHCKzJIfILMkhMkv6PxVqPDFMuBkdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(action)\n",
    "plt.imshow(observation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization du modèle (réseau de neurones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille de l'input, une grille de 80x80\n",
    "D = 80 * 80\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D)\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "# ??? (update buffers that add up gradients over a batch)\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } \n",
    "# Optimisation (rmsprop memory)\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction sigmoïd pour réduire les valeurs à l'interval [0,1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réduit l'image du jeu à un vecteur de 6400 états (pour une grilles de 80x80) avec pour valeurs 0 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount rewards\n",
    "\n",
    "\"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "\n",
    "reset the sum, since this was a game boundary (pong specific!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques déclarations de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "# used in computing the difference frame\n",
    "prev_x = None\n",
    "xs, hs, dlogps, drs = [], [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess the observation, set input to network to be difference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_image():\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward the policy network and sample an action from the returned probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(x):\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "    return action, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record various intermediates (needed later for backprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_history(x, h):\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    while timestep in range(50000):\n",
    "        if render: env.render()\n",
    "\n",
    "        x = get_diff_image()\n",
    "\n",
    "        action, h = get_action(x)\n",
    "\n",
    "        y = record_history(x, h)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            eph = np.vstack(hs)\n",
    "            epdlogp = np.vstack(dlogps)\n",
    "            epr = np.vstack(drs)\n",
    "            xs, hs, dlogps, drs = [], [], [], [] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "            epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "            grad = policy_backward(eph, epdlogp)\n",
    "            for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "            # perform rmsprop parameter update every batch_size episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                for k,v in model.items():\n",
    "                    g = grad_buffer[k] # gradient\n",
    "                    rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                    model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                    grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "            # boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print('resetting env. episode reward total was %f. running mean: %f', reward_sum, running_reward)\n",
    "            if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "            if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "            print('ep %d: game finished, reward: %f', episode_number, reward,'' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "state = obs_to_state(env.reset())\n",
    "done = False\n",
    "while done is False:\n",
    "    env.render()\n",
    "    action = greedy_policy(state)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
